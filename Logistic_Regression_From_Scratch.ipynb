{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** A simple implementation of Logistic Regression from scratch using NumPy. This project demonstrates how binary classification works without using any machine learning libraries. Includes training, prediction, loss computation, gradient descent, and performance evaluation on sample datasets."
      ],
      "metadata": {
        "id": "_MB0KtNAQoct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XFOol-vbQi69"
      },
      "outputs": [],
      "source": [
        "# importing important libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic_Regression():\n",
        "\n",
        "    # declaring learning rate and number of iterations(hyperparameters)\n",
        "    def __init__(self, learning_rate, no_of_iterations):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.no_of_iterations = no_of_iterations\n",
        "\n",
        "\n",
        "    # fit function to train the model to some dataset\n",
        "    def fit(self, X, Y):\n",
        "        self.m, self.n = X.shape    #[m=rows, n=cols]\n",
        "\n",
        "        self.w = np.zeros(self.n)   # for the total number of features creating empty weights\n",
        "        self.b = 0                  # bias - initially 0\n",
        "        self.X = X                  # initiating features\n",
        "        self.Y = Y                  # intiating target\n",
        "\n",
        "        # implementing gradient descent for optimization of solution\n",
        "        for i in range(no_of_iterations):\n",
        "            self.update_weights()\n",
        "\n",
        "    def update_weights(self):\n",
        "        # y_cap formula(sigmoid formula)\n",
        "        Y_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))  #z = wx+b\n",
        "\n",
        "        # Now build derivatives\n",
        "        dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
        "        db = (1/self.m)*np.sum(Y_hat = self.Y)\n",
        "\n",
        "        # updating weights and bias using gradient descent\n",
        "        self.w = self.w - self.learning_rate * dw\n",
        "        self.b = self.b - self.learning_rate * db\n",
        "\n",
        "    # sigmoid equation and decision boundary\n",
        "    def predict(self):\n",
        "         Y_pred = 1 / (1+ np.exp(-(self.X.dot(self.w) + self.b)))\n",
        "         Y_pred = np.where(Y_pred>0.5, 1, 0)\n",
        "         return Y_pred"
      ],
      "metadata": {
        "id": "h3So6dXSRUdB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ed4guPFVX8Ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}